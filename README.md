# Laboratorio-3
# Librer칤as utilizadas a lo largo del laboratorio 

```phyton
import librosa
import matplotlib.pyplot as plt
import numpy as np
```

# 1.Configuraci칩n del sistema
Como primer paso para configurar el sistema se ubicaron tres micr칩fonos:
Estos micr칩fonos se ubicaron a 6 m de distancia entre ellos,como se muestra en el siguiente bosquejo:



![image](https://github.com/user-attachments/assets/df980857-d875-415e-ad2c-4b8ce2195409)
# 2.Captura de la se침al.

1.Se grabaron tres se침ales mediante la voz de tres personas; las frases que dijeron en la captura fueron las siguientes:


Persona 1(Paula Vanessa):

游꿚 [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO1.wav)

Persona 2(Mar칤a Paula):"Mi comida favorita es la hamburguesa sin verduras."

游꿚 [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO2.wav)

Persona 1(Juan Pablo:"Las medusas existen desde hace ma패s de 600 millones de an팪os lo que las hace ma패s antiguas que los dinosaurios los tiburones e incluso맓os마패rboles."

游꿚 [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO3.wav)

Estas frases fueron grabadas al tiempo y se almacenaron en los dispositivos en archivos .wav.

2.Luego se capturaron las se침ales del ruido de fondo con los tres micr칩fonos.

Ruido de fondo 1:

游꿚 [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO1.wav)

Ruido de fondo 2:

游꿚 [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO2.wav)

Ruido de fondo 3:

游꿚 [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO3.wav)

# Se침ales audio-fondo y c치lculo de SNR (1) 
```python
audio_path1 = 'AUDIO1.wav'
audio_path2 = 'FONDO1.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

plt.figure(figsize=(12, 6))

# Primera se침al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 1", alpha=0.7)
plt.title('Forma de onda de AUDIO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se침al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 1", color='orange', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

AUDIO1 = np.mean(y1 ** 2)
FONDO1 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO1 / FONDO1)

print(f"SNR: {snr_db:.2f} dB")
````
![image](https://github.com/user-attachments/assets/2c4374f9-2675-46d1-b46e-4eddc2325b2e)

SNR:38.29

# Se침ales audio-fondo y c치lculo de SNR (2) 

# Cargar los archivos de audio

```phyton
audio_path1 = 'AUDIO2.wav'
audio_path2 = 'FONDO2.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

# Crear los ejes de tiempo
time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

# Graficar ambas se침ales
plt.figure(figsize=(12, 6))

# Primera se침al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 2", alpha=0.7)
plt.title('Forma de onda de AUDIO 2')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se침al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 2", color='red', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 2')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

# Calcular la SNR en dB (Se침al: y1, Ruido: y2)
AUDIO2 = np.mean(y1 ** 2)
FONDO2 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO2 / FONDO2)

# Separaci칩n vertical
offset = 2
y2_shifted = y2 + offset

time = np.arange(len(y1)) / sr1

print(f"SNR: {snr_db:.2f} dB")
```
![image](https://github.com/user-attachments/assets/6ef3002a-2e6d-4c74-88ee-14cc33946a79)

SNR:37.43

# Se침ales audio-fondo y c치lculo de SNR (2) 


```phyton
audio_path1 = 'AUDIO3.wav'
audio_path2 = 'FONDO3.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

# Crear los ejes de tiempo
time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

# Graficar ambas se침ales
plt.figure(figsize=(12, 6))

# Primera se침al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 3", alpha=0.7)
plt.title('Forma de onda de AUDIO 3')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se침al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 3", color='green', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

# Calcular la SNR en dB (Se침al: y1, Ruido: y2)
AUDIO3 = np.mean(y1 ** 2)
FONDO3 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO3 / FONDO3)

# Separaci칩n vertical
offset = 2
y2_shifted = y2 + offset

time = np.arange(len(y1)) / sr1

print(f"SNR: {snr_db:.2f} dB")
```
![image](https://github.com/user-attachments/assets/cfc70543-ca08-4083-b598-1d8097ceba38)

SNR:26.89

# 3.Analisis temporal y espectral de las se침ales capturadas por los micr칩fonos.

Caracteristicas principales de cada fuente sonora:



An치lisis y gr치fica de un audio:

El an치lisis y graficaci칩n de audio permiten examinar una se침al en el dominio del tiempo y la frecuencia para comprender sus caracter칤sticas.

-Un archivo de audio se convierte en una serie de valores num칠ricos que representan su variaci칩n en el tiempo.

-An치lisis temporal: La forma de onda muestra c칩mo cambia la amplitud del sonido, permitiendo identificar eventos como golpes o pausas.

-An치lisis frecuencial: La Transformada R치pida de Fourier (FFT) convierte la se침al al dominio de la frecuencia, mostrando qu칠 frecuencias est치n presentes y su intensidad en decibeles (dB).

-Permite identificar sonidos graves o agudos, analizar ruido y comparar se침ales de distintos micr칩fonos.

```phyton
audio_paths = ['AUDIO1.wav', 'AUDIO2.wav', 'AUDIO3.wav']
signals = []
sampling_rates = []

for path in audio_paths:
    y, sr = librosa.load(path, sr=None)  # Mantener frecuencia original
    signals.append(y)
    sampling_rates.append(sr)

# An치lisis Temporal: Graficar las formas de onda
plt.figure(figsize=(12, 8))
for i, y in enumerate(signals):
    plt.subplot(3, 1, i+1)
    librosa.display.waveshow(y, sr=sampling_rates[i], alpha=0.7)
    plt.title(f'Forma de onda - Micr칩fono {i+1}')
    plt.xlabel('Tiempo (s)')
    plt.ylabel('Amplitud (dB)')
    plt.xticks(np.linspace(0, len(y) / sampling_rates[i], num=5))  # Etiquetas en el eje x
    plt.yticks(np.linspace(np.min(y), np.max(y), num=5))  # Etiquetas en el eje y

plt.tight_layout()
plt.show()

# An치lisis Espectral: Aplicar FFT y visualizar espectros de frecuencia
plt.figure(figsize=(12, 8))
for i, y in enumerate(signals):
    N = len(y)  # N칰mero de muestras
    T = 1.0 / sampling_rates[i]  # Per칤odo de muestreo
    freqs = np.fft.rfftfreq(N, T)  # Frecuencias
    fft_vals = 20 * np.log10(np.abs(np.fft.rfft(y)) + 1e-6)  # Convertir amplitud a dB  # Magnitud de la FFT

    plt.subplot(3, 1, i+1)
    plt.plot(freqs, fft_vals, label=f'Espectro - Micr칩fono {i+1}')
    plt.xlim(0, 5000)  # Limitar a 5 kHz (frecuencias de voz humana)
    plt.xlabel('Frecuencia (Hz)')
    plt.ylabel('Amplitud (dB)')
    plt.xticks(np.linspace(0, 5000, num=6))  # Etiquetas en el eje x
    plt.yticks(np.linspace(np.min(fft_vals), np.max(fft_vals), num=5))  # Etiquetas en el eje y
    plt.legend()

plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/4db6ddca-792b-413f-8cf3-adb39022b9dc)


# 3.M칠todos de separaci칩n de fuentes
 # ICA
 
# 4.SNR

Es una medida que compara el nivel de una se침al deseada con el nivel del ruido de fondo en un sistema de audio o comunicaciones. Se expresa generalmente en decibeles (dB)
                                                                                               SNR=10log10(Pse침al/Pruido) 

Un SNR alto (mayor cantidad de dB) significa que la se침al es mucho m치s fuerte que el ruido, lo que resulta en una mejor calidad de audio, mientras que un SNR bajo (menor cantidad de dB) indica que el ruido es comparable o incluso mayor que la se침al, lo que puede dificultar la interpretaci칩n de la informaci칩n transmitida.





# 5 Preguntas a resolver
-쮺칩mo afecta la posici칩n relativa de los micr칩fonos y las fuentes sonoras en la efectividad de la separaci칩n de se침ales?

La posici칩n relativa de los micr칩fonos y las fuentes sonoras afecta significativamente la efectividad en la separaci칩n de se침ales debido a la distancia, la direccionalidad y el rechazo de interferencias.

Distancia y 치ngulo: Un mayor espaciamiento entre micr칩fonos mejora la capacidad de separar se침ales basadas en la diferencia de tiempo de llegada y la fase.

Rechazo de ruido: Un correcto posicionamiento minimiza interferencias y maximiza la captaci칩n de la se침al deseada.

Efecto de diafon칤a: Si las fuentes est치n muy cercanas o alineadas, puede ser m치s dif칤cil separar sus se침ales debido a la superposici칩n de frecuencias.

Ambiente ac칰stico: La reverberaci칩n y los reflejos pueden alterar la captaci칩n, por lo que la ubicaci칩n de los micr칩fonos debe optimizarse para minimizar estos efectos.

-쯈u칠 mejoras implementar칤a en la metodolog칤a para obtener mejores resultados?

Optimizaci칩n de la disposici칩n espacial

Aumentar la separaci칩n entre micr칩fonos para mejorar la diferenciaci칩n de las se침ales seg칰n la diferencia de tiempo de llegada.
Ubicar los micr칩fonos estrat칠gicamente para minimizar interferencias y reflexiones no deseadas.

Reducci칩n de ruido y diafon칤a

Utilizar micr칩fonos direccionales o configuraciones de matrices para mejorar la captaci칩n de la se침al deseada y reducir interferencias.
Implementar filtros de acondicionamiento de se침ales para minimizar el ruido ambiental.

Calibraci칩n y caracterizaci칩n precisa

Realizar pruebas previas de caracterizaci칩n de los micr칩fonos para identificar su respuesta en diferentes condiciones.
Aplicar t칠cnicas de normalizaci칩n y ajuste de ganancia para mantener niveles de se침al 칩ptimos.

Mejora en el procesamiento de se침ales

Aplicar t칠cnicas avanzadas de separaci칩n de fuentes como algoritmos de an치lisis de componentes independientes (ICA) o filtrado adaptativo.
Implementar modelos de machine learning o t칠cnicas de deep learning para mejorar la separaci칩n de se침ales en entornos ruidosos.

Simulaci칩n y pruebas experimentales

Realizar simulaciones previas mediante software como MATLAB o Proteus para ajustar los par치metros del sistema antes de la implementaci칩n real.
Efectuar mediciones en diferentes condiciones para evaluar la robustez del m칠todo y ajustar los par치metros necesarios.
