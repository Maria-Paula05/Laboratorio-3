# Laboratorio 3 - Problema del c√≥ctel 

Este laboratorio aborda el problema del c√≥ctel, que consiste en aislar una fuente de sonido en un entorno con m√∫ltiples emisores. A lo largo del repositorio, se implementan t√©cnicas de procesamiento digital de se√±ales, como el An√°lisis de Componentes Independientes (ICA) y Beamforming, para separar las se√±ales capturadas por un arreglo de micr√≥fonos y evaluar experimentalmente la efectividad de cada m√©todo.

# 1. Configuraci√≥n del sistema
Como primer paso para configurar el sistema se ubicaron tres micr√≥fonos:
Estos micr√≥fonos se ubicaron a 6 m de distancia entre ellos,como se muestra en el siguiente bosquejo:



![image](https://github.com/user-attachments/assets/df980857-d875-415e-ad2c-4b8ce2195409)
# 2. Captura de la se√±al.

1. Se grabaron tres se√±ales mediante la voz de tres personas; las frases que dijeron en la captura fueron las siguientes:

Persona 1(Paula Vanessa):"Por accidente le pegue a la ventana de la vecina y la romp√≠"

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO1.wav)

Persona 2(Mar√≠a Paula):"Mi comida favorita es la hamburguesa sin verduras."

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO2.wav)

Persona 1(Juan Pablo):"Las medusas existen desde hace maÃÅs de 600 millones de anÃÉos lo que las hace maÃÅs antiguas que los dinosaurios los tiburones e incluso¬†los¬†aÃÅrboles."

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO3.wav)

Estas frases fueron grabadas al tiempo y se almacenaron en los dispositivos en archivos .wav.

2. Luego se capturaron las se√±ales del ruido de fondo con los tres micr√≥fonos.

Ruido de fondo 1:

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO1.wav)

Ruido de fondo 2:

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO2.wav)

Ruido de fondo 3:

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO3.wav)

# Se√±ales audio-fondo y c√°lculo de SNR (1) 
```python
audio_path1 = 'AUDIO1.wav'
audio_path2 = 'FONDO1.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

plt.figure(figsize=(12, 6))

# Primera se√±al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 1", alpha=0.7)
plt.title('Forma de onda de AUDIO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se√±al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 1", color='orange', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

AUDIO1 = np.mean(y1 ** 2)
FONDO1 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO1 / FONDO1)

print(f"SNR: {snr_db:.2f} dB")
````
![image](https://github.com/user-attachments/assets/2c4374f9-2675-46d1-b46e-4eddc2325b2e)

SNR:38.29

# Se√±ales audio-fondo y c√°lculo de SNR (2) 

```python
audio_path1 = 'AUDIO2.wav'
audio_path2 = 'FONDO2.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

# Crear los ejes de tiempo
time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

# Graficar ambas se√±ales
plt.figure(figsize=(12, 6))

# Primera se√±al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 2", alpha=0.7)
plt.title('Forma de onda de AUDIO 2')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se√±al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 2", color='red', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 2')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

# Calcular la SNR en dB (Se√±al: y1, Ruido: y2)
AUDIO2 = np.mean(y1 ** 2)
FONDO2 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO2 / FONDO2)

# Separaci√≥n vertical
offset = 2
y2_shifted = y2 + offset

time = np.arange(len(y1)) / sr1

print(f"SNR: {snr_db:.2f} dB")
```
![image](https://github.com/user-attachments/assets/6ef3002a-2e6d-4c74-88ee-14cc33946a79)

SNR:37.43

# Se√±ales audio-fondo y c√°lculo de SNR (3) 


```python
audio_path1 = 'AUDIO3.wav'
audio_path2 = 'FONDO3.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

# Crear los ejes de tiempo
time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

# Graficar ambas se√±ales
plt.figure(figsize=(12, 6))

# Primera se√±al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 3", alpha=0.7)
plt.title('Forma de onda de AUDIO 3')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se√±al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 3", color='green', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

# Calcular la SNR en dB (Se√±al: y1, Ruido: y2)
AUDIO3 = np.mean(y1 ** 2)
FONDO3 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO3 / FONDO3)

# Separaci√≥n vertical
offset = 2
y2_shifted = y2 + offset

time = np.arange(len(y1)) / sr1

print(f"SNR: {snr_db:.2f} dB")
```
![image](https://github.com/user-attachments/assets/cfc70543-ca08-4083-b598-1d8097ceba38)

SNR:26.89

# 3. Analisis temporal y espectral de las se√±ales capturadas por los micr√≥fonos.

**Caracteristicas principales de cada fuente sonora:**

En este caso, se tienen tres fuentes sonoras:

1. Voces humanas
   
Cada una de las tres voces tendr√° caracter√≠sticas distintas, dependiendo de la persona que habla:

- Timbre: Cada voz tiene un tono √∫nico (grave, agudo, nasal, suave, √°spera, etc.).

- Intensidad: Algunas voces pueden sonar m√°s fuertes o m√°s suaves seg√∫n la distancia al micr√≥fono o la manera de hablar.

- Velocidad y ritmo: Algunas personas pueden hablar r√°pido, pausado, con √©nfasis en ciertas palabras.

- Entonaci√≥n: Puede haber variaciones seg√∫n la emoci√≥n (tono serio, alegre, enojado, triste, etc.).

- Ubicaci√≥n espacial: En un audio est√©reo, las voces pueden ubicarse en distintos puntos (izquierda, derecha o centro).

2. M√∫sica de fondo
   
La m√∫sica tambi√©n tiene varias caracter√≠sticas que afectan c√≥mo se percibe en el audio:

- Timbre: Depender√° de los instrumentos utilizados (piano, guitarra, sintetizador, etc.).

- Volumen: Puede ser baja para no opacar las voces o alta si es el elemento principal en algunos momentos.

- Ritmo y tempo: Puede ser r√°pida, lenta, con cambios de velocidad.

- Estilo: Puede ser cl√°sica, electr√≥nica, rock, jazz, etc., lo que influye en la atm√≥sfera del audio.

- Presencia: Puede sonar de fondo de manera constante o variar su intensidad en diferentes momentos.

3. Efectos de sonido 
Los efectos de sonido pueden aportar realismo o dramatismo al audio:

- Naturales: Sonidos de la universidad, viento, pasos, murmullos de fondo.

- Artificiales: Transiciones, ruidos a√±adidos en edici√≥n, eco, reverberaci√≥n.

- Ubicaci√≥n espacial: Al igual que las voces, pueden posicionarse en distintos puntos del est√©reo (izquierda, derecha, fondo).

- Prop√≥sito: Pueden ser utilizados para ambientar, enfatizar acciones o mejorar la experiencia sonora.

An√°lisis y gr√°fica de un audio: El an√°lisis y graficaci√≥n de audio permiten examinar una se√±al en el dominio del tiempo y la frecuencia para comprender sus caracter√≠sticas.

- An√°lisis temporal: La forma de onda muestra c√≥mo cambia la amplitud del sonido, permitiendo identificar eventos como golpes o pausas.

```python
audio_paths = ['AUDIO1.wav', 'AUDIO2.wav', 'AUDIO3.wav']
signals = []
sampling_rates = []

for path in audio_paths:
    y, sr = librosa.load(path, sr=None)  # Mantener frecuencia original
    signals.append(y)
    sampling_rates.append(sr)

# An√°lisis Temporal: Graficar las formas de onda
plt.figure(figsize=(12, 8))
for i, y in enumerate(signals):
    plt.subplot(3, 1, i+1)
    librosa.display.waveshow(y, sr=sampling_rates[i], alpha=0.7)
    plt.title(f'Forma de onda - Micr√≥fono {i+1}')
    plt.xlabel('Tiempo (s)')
    plt.ylabel('Amplitud (dB)')
    plt.xticks(np.linspace(0, len(y) / sampling_rates[i], num=5))  # Etiquetas en el eje x
    plt.yticks(np.linspace(np.min(y), np.max(y), num=5))  # Etiquetas en el eje y

plt.tight_layout()
plt.show()
```
![image](https://raw.githubusercontent.com/Maria-Paula05/Laboratorio-3/refs/heads/main/ONDA.png)

- An√°lisis frecuencial: La Transformada R√°pida de Fourier (FFT) convierte la se√±al al dominio de la frecuencia, mostrando qu√© frecuencias est√°n presentes y su intensidad en decibeles (dB).

    Permite identificar sonidos graves o agudos, analizar ruido y comparar se√±ales de distintos micr√≥fonos.

```python
# An√°lisis Espectral: Aplicar FFT y visualizar espectros de frecuencia
plt.figure(figsize=(12, 8))
for i, y in enumerate(signals):
    N = len(y)  # N√∫mero de muestras
    T = 1.0 / sampling_rates[i]  # Per√≠odo de muestreo
    freqs = np.fft.rfftfreq(N, T)  # Frecuencias
    fft_vals = 20 * np.log10(np.abs(np.fft.rfft(y)) + 1e-6)  # Convertir amplitud a dB  # Magnitud de la FFT

    plt.subplot(3, 1, i+1)
    plt.plot(freqs, fft_vals, label=f'Espectro - Micr√≥fono {i+1}')
    plt.xlim(0, 5000)  # Limitar a 5 kHz (frecuencias de voz humana)
    plt.xlabel('Frecuencia (Hz)')
    plt.ylabel('Amplitud (dB)')
    plt.xticks(np.linspace(0, 5000, num=6))  # Etiquetas en el eje x
    plt.yticks(np.linspace(np.min(fft_vals), np.max(fft_vals), num=5))  # Etiquetas en el eje y
    plt.legend()

plt.tight_layout()
plt.show()
```
![image](https://raw.githubusercontent.com/Maria-Paula05/Laboratorio-3/refs/heads/main/FFT.png)

# 4. M√©todos de separaci√≥n de fuentes
# ICA
*"El An√°lisis de Componentes Independientes (ICA) es una t√©cnica estad√≠stica que permite separar se√±ales mezcladas asumiendo que sus fuentes son estad√≠sticamente independientes. Se utiliza en el procesamiento de audio para aislar sonidos individuales a partir de una mezcla." (Learn Statistics Easily, 2024).*

El proceso para separar fuentes comienza con la carga de archivos de audio, asegurando que todas las se√±ales mezcladas tengan la misma frecuencia de muestreo y longitud. Luego, se aplica el An√°lisis de Componentes Independientes (ICA) para separar las fuentes sonoras, asumiendo que son estad√≠sticamente independientes.
Para mejorar la calidad del audio extra√≠do, se utiliza un filtro pasa banda (100 Hz - 4000 Hz), eliminando frecuencias no deseadas. Luego, la se√±al se normaliza y convierte al formato int16, optimiz√°ndola para su almacenamiento. Finalmente, el archivo se guarda en formato .wav.

```python
import librosa
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import FastICA
import soundfile as sf

# Cargar el audio
audio_path1 = 'AUDIO3.wav'
y1, sr1 = librosa.load(audio_path1, mono=True)
fs = sr1 
duration = len(y1) / fs

# Asegurar que la se√±al sea de dos canales para ICA
y2 = np.roll(y1, 1000)
audios_mixtos = np.column_stack([y1, y2])

# Aplicar FastICA
ica = FastICA(n_components=2)
audios_separados = ica.fit_transform(audios_mixtos)

# Seleccionar la primera fuente estimada
senal_aislada = audios_separados[:, 0]

# Ajustar amplitudes al mismo nivel
senal_aislada /= np.max(np.abs(senal_aislada))  # Normalizar entre [-1,1]
senal_aislada *= np.max(np.abs(y1))  # Escalar a la misma amplitud que AUDIO3

# Guardar la se√±al aislada
sf.write("VOZ3.wav", senal_aislada, fs)
print("Se guard√≥ la se√±al aislada con ICA en 'VOZ3.wav'.")

# C√°lculo de SNR basado en la se√±al original como referencia
def calculate_snr(original, isolated):
    original = original[:len(isolated)]  # Asegurar mismo tama√±o
    noise = original - isolated  # Calcular ruido
    signal_power = np.mean(original ** 2)
    noise_power = np.mean(noise ** 2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    return snr

snr_aislada = calculate_snr(y1, senal_aislada)
print(f"SNR de AUDIO3 aislado: {snr_aislada:.2f} dB")

# Graficar AUDIO3 original y aislado
plt.figure(figsize=(12, 6))
plt.subplot(2, 1, 1)
plt.plot(np.linspace(0, duration, len(y1)), y1, color="green")
plt.xlabel("Tiempo (s)")
plt.ylabel("Amplitud (Normalizada)")
plt.title("AUDIO3 Original")

plt.subplot(2, 1, 2)
plt.plot(np.linspace(0, duration, len(senal_aislada)), senal_aislada, color="blue")
plt.xlabel("Tiempo (s)")
plt.ylabel("Amplitud (Normalizada)")
plt.title("AUDIO3 Aislado")

plt.tight_layout()
plt.show()

```
**Comparaci√≥n se√±al aislada / la se√±al original**

Por ultimo se compara la se√±al aislada con la se√±al original mediante m√©tricas de calidad, para cuantificar el desempe√±o de la separaci√≥n y evaluar la efectividad del proceso.


*Se guard√≥ la se√±al aislada con ICA en 'VOZ3.wav'.*

*SNR de AUDIO3 aislado: 16.92 dB*
![image](https://raw.githubusercontent.com/Maria-Paula05/Laboratorio-3/refs/heads/main/AISLADA.png)

üéß [AUDIO 3](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO3.wav)
üéß [VOZ3](VOZ3.wav)

# Beamforming
*"Beamforming es una t√©cnica que se utiliza para mejorar la relaci√≥n se√±al-ruido de las se√±ales recibidas, eliminar fuentes de interferencia no deseadas y enfocar se√±ales transmitidas a ubicaciones espec√≠ficas." (Beamforming, s.f.)*

```python

folder_path = "C:\\Users\\paula\\Desktop\\COCTEL"

# üé§ Cargar audios de los micr√≥fonos
fs, mic1 = wav.read(os.path.join(folder_path, "AUDIO1.wav"))
_, mic2 = wav.read(os.path.join(folder_path, "AUDIO2.wav"))
_, mic3 = wav.read(os.path.join(folder_path, "AUDIO3.wav"))

signals = np.vstack([mic1, mic2, mic3])

# Posiciones actualizadas de los micr√≥fonos seg√∫n la imagen
mic_positions = np.array([
    [-3, 6],  # Mic 1 (izquierda)
    [3, 6],   # Mic 2 (derecha)
    [0, -1],  # Mic 3 (abajo en el centro)
]).T  # Transpuesta para el formato correcto

# Direcci√≥n de la persona central (asumimos que est√° en (0,0))
angle = np.pi / 2  # 90¬∞

# üõ† Crear beamformer Delay-and-Sum
R = mic_positions  # Matriz de posiciones
beamformer = pra.beamforming.DelayAndSum(R, fs, nfft=256)  # Intentar con DelayAndSum

# Apuntar el beamformer a la direcci√≥n deseada
beamformer.steer(azimuth=angle, colatitude=np.pi/2)  

# Aplicar beamforming
output_signal = beamformer.process(signals)

# Guardar el audio resultante
output_path = os.path.join(folder_path, "voz_central_beamforming.wav")
wav.write(output_path, fs, output_signal.astype(np.int16))

print(f"Audio procesado guardado en: {output_path}")

```

# SNR

Es una medida que compara el nivel de una se√±al deseada con el nivel del ruido de fondo en un sistema de audio o comunicaciones. Se expresa generalmente en decibeles (dB)
                                                                                               SNR=10log10(Pse√±al/Pruido)
                                                                                               
Un SNR alto (mayor cantidad de dB) significa que la se√±al es mucho m√°s fuerte que el ruido, lo que resulta en una mejor calidad de audio, mientras que un SNR bajo (menor cantidad de dB) indica que el ruido es comparable o incluso mayor que la se√±al, lo que puede dificultar la interpretaci√≥n de la informaci√≥n transmitida.

# 5. Preguntas a resolver
**¬øC√≥mo afecta la posici√≥n relativa de los micr√≥fonos y las fuentes sonoras en la efectividad de la separaci√≥n de se√±ales?**

- Posici√≥n relativa de los micr√≥fonos: la posici√≥n de los micr√≥fonos y las fuentes sonoras afecta significativamente la efectividad en la separaci√≥n de se√±ales debido a la distancia, la direccionalidad y el rechazo de interferencias.

- Distancia y √°ngulo: Un mayor espaciamiento entre micr√≥fonos mejora la capacidad de separar se√±ales basadas en la diferencia de tiempo de llegada y la fase.

- Rechazo de ruido: Un correcto posicionamiento minimiza interferencias y maximiza la captaci√≥n de la se√±al deseada.

- Efecto de diafon√≠a: Si las fuentes est√°n muy cercanas o alineadas, puede ser m√°s dif√≠cil separar sus se√±ales debido a la superposici√≥n de frecuencias.

- Ambiente ac√∫stico: La reverberaci√≥n y los reflejos pueden alterar la captaci√≥n, por lo que la ubicaci√≥n de los micr√≥fonos debe optimizarse para minimizar estos efectos.

**¬øQu√© mejoras implementar√≠a en la metodolog√≠a para obtener mejores resultados?**

- Optimizaci√≥n de la disposici√≥n espacial

    Aumentar la separaci√≥n entre micr√≥fonos para mejorar la diferenciaci√≥n de las se√±ales seg√∫n la diferencia de tiempo de llegada.
    Ubicar los micr√≥fonos estrat√©gicamente para minimizar interferencias y reflexiones no deseadas.

- Reducci√≥n de ruido y diafon√≠a

    Utilizar micr√≥fonos direccionales o configuraciones de matrices para mejorar la captaci√≥n de la se√±al deseada y reducir interferencias.
    Implementar filtros de acondicionamiento de se√±ales para minimizar el ruido ambiental.

- Calibraci√≥n y caracterizaci√≥n precisa

    Realizar pruebas previas de caracterizaci√≥n de los micr√≥fonos para identificar su respuesta en diferentes condiciones.
    Aplicar t√©cnicas de normalizaci√≥n y ajuste de ganancia para mantener niveles de se√±al √≥ptimos.

- Mejora en el procesamiento de se√±ales

    Aplicar t√©cnicas avanzadas de separaci√≥n de fuentes como algoritmos de an√°lisis de componentes independientes (ICA) o filtrado adaptativo.
    Implementar modelos de machine learning o t√©cnicas de deep learning para mejorar la separaci√≥n de se√±ales en entornos ruidosos.

- Simulaci√≥n y pruebas experimentales

    Realizar simulaciones previas mediante software como MATLAB o Proteus para ajustar los par√°metros del sistema antes de la implementaci√≥n real.
    Efectuar mediciones en diferentes condiciones para evaluar la robustez del m√©todo y ajustar los par√°metros necesarios.

# 6. Conclusiones
- La resoluci√≥n experimental del problema del c√≥ctel demostr√≥ que es posible aislar parcialmente una voz espec√≠fica entre m√∫ltiples fuentes sonoras. Para ello, implementamos dos t√©cnicas: An√°lisis de Componentes Independientes (ICA) y Beamforming. Los resultados indicaron que ICA logr√≥ una separaci√≥n m√°s clara, aunque a√∫n incompleta, de la voz objetivo.

- El principal obst√°culo fue la calidad de la captura de audio. Factores como la disposici√≥n inadecuada de los celulares, variaciones en la ganancia y condiciones ac√∫sticas desfavorables afectaron los datos iniciales, limitando la efectividad del procesamiento posterior.

- El mejor desempe√±o de ICA en comparaci√≥n con Beamforming sugiere que la calidad de los datos de entrada fue un factor determinante en los resultados, m√°s que la t√©cnica de procesamiento utilizada.
  
# Referencias

-   Transformaci√≥n r√°pida de Fourier FFT (s.f.).
https://www.nti-audio.com/es/servicio/conocimientos/transformacion-rapida-de-fourier-fft

-   Rivera, E., Moreno, R., P√©rez, H., & Nakano, M. (2020). Separaci√≥n de se√±ales usando an√°lisis de componentes principales y muestreo compresivo con mediciones m√≠nimas. CIT Informacion Tecnologica, 31(1), 287‚Äì300. https://doi.org/10.4067/s0718-07642020000100287

-   Acaro, X., Molina, M., Corapi, P., Molina Villacis, M., Reamache Rivera, G. J., & Castillo Garc√≠a, J. V. (2021). Interfaz gr√°fica para el an√°lisis de audio y sonidos urbanos (CasRem). Investigaci√≥n, Tecnolog√≠a e Innovaci√≥n, 13(14), 29‚Äì42. https://doi.org/10.53591/iti.v13i14.1308

-   Marengo Rodriguez, F. A., Roveri, E. A., Guerrero, J. M. R., Treffil√≥, M., & Miyara, F. (s.f). AN√ÅLISIS COMPARATIVO DE CODIFICADORES DE AUDIO SIN P√âRDIDAS. Edu.ar. Recuperado el 28 de febrero de 2025, de https://www.fceia.unr.edu.ar/acustica/codecdigital/archivos/comparativo-codificadores-sin-perdidas-UNTREF.pdf

-   Beamforming. (s.f.). MATLAB & Simulink. https://la.mathworks.com/discovery/beamforming.html
