# Laboratorio-3
# Librer√≠as utilizadas a lo largo del laboratorio 

```python
import librosa
import matplotlib.pyplot as plt
import numpy as np
```

# 1. Configuraci√≥n del sistema
Como primer paso para configurar el sistema se ubicaron tres micr√≥fonos:
Estos micr√≥fonos se ubicaron a 6 m de distancia entre ellos,como se muestra en el siguiente bosquejo:



![image](https://github.com/user-attachments/assets/df980857-d875-415e-ad2c-4b8ce2195409)
# 2. Captura de la se√±al.

1. Se grabaron tres se√±ales mediante la voz de tres personas; las frases que dijeron en la captura fueron las siguientes:

Persona 1(Paula Vanessa):"Por accidente le pegue a la ventana de la vecina y la romp√≠"

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO1.wav)

Persona 2(Mar√≠a Paula):"Mi comida favorita es la hamburguesa sin verduras."

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO2.wav)

Persona 1(Juan Pablo):"Las medusas existen desde hace maÃÅs de 600 millones de anÃÉos lo que las hace maÃÅs antiguas que los dinosaurios los tiburones e incluso¬†los¬†aÃÅrboles."

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/AUDIO3.wav)

Estas frases fueron grabadas al tiempo y se almacenaron en los dispositivos en archivos .wav.

2. Luego se capturaron las se√±ales del ruido de fondo con los tres micr√≥fonos.

Ruido de fondo 1:

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO1.wav)

Ruido de fondo 2:

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO2.wav)

Ruido de fondo 3:

üéß [Escuchar el audio](https://github.com/Maria-Paula05/Laboratorio-3/blob/main/FONDO3.wav)

# Se√±ales audio-fondo y c√°lculo de SNR (1) 
```python
audio_path1 = 'AUDIO1.wav'
audio_path2 = 'FONDO1.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

plt.figure(figsize=(12, 6))

# Primera se√±al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 1", alpha=0.7)
plt.title('Forma de onda de AUDIO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se√±al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 1", color='orange', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

AUDIO1 = np.mean(y1 ** 2)
FONDO1 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO1 / FONDO1)

print(f"SNR: {snr_db:.2f} dB")
````
![image](https://github.com/user-attachments/assets/2c4374f9-2675-46d1-b46e-4eddc2325b2e)

SNR:38.29

# Se√±ales audio-fondo y c√°lculo de SNR (2) 

```python
audio_path1 = 'AUDIO2.wav'
audio_path2 = 'FONDO2.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

# Crear los ejes de tiempo
time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

# Graficar ambas se√±ales
plt.figure(figsize=(12, 6))

# Primera se√±al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 2", alpha=0.7)
plt.title('Forma de onda de AUDIO 2')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se√±al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 2", color='red', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 2')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

# Calcular la SNR en dB (Se√±al: y1, Ruido: y2)
AUDIO2 = np.mean(y1 ** 2)
FONDO2 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO2 / FONDO2)

# Separaci√≥n vertical
offset = 2
y2_shifted = y2 + offset

time = np.arange(len(y1)) / sr1

print(f"SNR: {snr_db:.2f} dB")
```
![image](https://github.com/user-attachments/assets/6ef3002a-2e6d-4c74-88ee-14cc33946a79)

SNR:37.43

# Se√±ales audio-fondo y c√°lculo de SNR (3) 


```python
audio_path1 = 'AUDIO3.wav'
audio_path2 = 'FONDO3.wav'
y1, sr1 = librosa.load(audio_path1)
y2, sr2 = librosa.load(audio_path2)

# Crear los ejes de tiempo
time1 = np.linspace(0, len(y1) / sr1, num=len(y1))
time2 = np.linspace(0, len(y2) / sr2, num=len(y2))

# Graficar ambas se√±ales
plt.figure(figsize=(12, 6))

# Primera se√±al (AUDIO 1)
plt.subplot(2, 1, 1)
plt.plot(time1, y1, label="Forma de onda AUDIO 3", alpha=0.7)
plt.title('Forma de onda de AUDIO 3')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

# Segunda se√±al (RUIDO DE FONDO 1)
plt.subplot(2, 1, 2)
plt.plot(time2, y2, label="Forma de onda RUIDO DE FONDO 3", color='green', alpha=0.7)
plt.title('Forma de onda de RUIDO DE FONDO 1')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud (normalizada)')
plt.legend()

plt.tight_layout()
plt.show()

min_len = min(len(y1), len(y2))
y1, y2 = y1[:min_len], y2[:min_len]

# Calcular la SNR en dB (Se√±al: y1, Ruido: y2)
AUDIO3 = np.mean(y1 ** 2)
FONDO3 = np.mean(y2 ** 2)
snr_db = 10 * np.log10(AUDIO3 / FONDO3)

# Separaci√≥n vertical
offset = 2
y2_shifted = y2 + offset

time = np.arange(len(y1)) / sr1

print(f"SNR: {snr_db:.2f} dB")
```
![image](https://github.com/user-attachments/assets/cfc70543-ca08-4083-b598-1d8097ceba38)

SNR:26.89

# 3. Analisis temporal y espectral de las se√±ales capturadas por los micr√≥fonos.

**Caracteristicas principales de cada fuente sonora:**

En este caso, se tienen tres fuentes sonoras:

1. Voces humanas
   
Cada una de las tres voces tendr√° caracter√≠sticas distintas, dependiendo de la persona que habla:

- Timbre: Cada voz tiene un tono √∫nico (grave, agudo, nasal, suave, √°spera, etc.).

- Intensidad: Algunas voces pueden sonar m√°s fuertes o m√°s suaves seg√∫n la distancia al micr√≥fono o la manera de hablar.

- Velocidad y ritmo: Algunas personas pueden hablar r√°pido, pausado, con √©nfasis en ciertas palabras.

- Entonaci√≥n: Puede haber variaciones seg√∫n la emoci√≥n (tono serio, alegre, enojado, triste, etc.).

- Ubicaci√≥n espacial: En un audio est√©reo, las voces pueden ubicarse en distintos puntos (izquierda, derecha o centro).

2. M√∫sica de fondo
   
La m√∫sica tambi√©n tiene varias caracter√≠sticas que afectan c√≥mo se percibe en el audio:

- Timbre: Depender√° de los instrumentos utilizados (piano, guitarra, sintetizador, etc.).

- Volumen: Puede ser baja para no opacar las voces o alta si es el elemento principal en algunos momentos.

- Ritmo y tempo: Puede ser r√°pida, lenta, con cambios de velocidad.

- Estilo: Puede ser cl√°sica, electr√≥nica, rock, jazz, etc., lo que influye en la atm√≥sfera del audio.

- Presencia: Puede sonar de fondo de manera constante o variar su intensidad en diferentes momentos.

3. Efectos de sonido 
Los efectos de sonido pueden aportar realismo o dramatismo al audio:

- Naturales: Sonidos de la universidad, viento, pasos, murmullos de fondo.

- Artificiales: Transiciones, ruidos a√±adidos en edici√≥n, eco, reverberaci√≥n.

- Ubicaci√≥n espacial: Al igual que las voces, pueden posicionarse en distintos puntos del est√©reo (izquierda, derecha, fondo).

- Prop√≥sito: Pueden ser utilizados para ambientar, enfatizar acciones o mejorar la experiencia sonora.

An√°lisis y gr√°fica de un audio: El an√°lisis y graficaci√≥n de audio permiten examinar una se√±al en el dominio del tiempo y la frecuencia para comprender sus caracter√≠sticas.

- An√°lisis temporal: La forma de onda muestra c√≥mo cambia la amplitud del sonido, permitiendo identificar eventos como golpes o pausas.

```python
audio_paths = ['AUDIO1.wav', 'AUDIO2.wav', 'AUDIO3.wav']
signals = []
sampling_rates = []

for path in audio_paths:
    y, sr = librosa.load(path, sr=None)  # Mantener frecuencia original
    signals.append(y)
    sampling_rates.append(sr)

# An√°lisis Temporal: Graficar las formas de onda
plt.figure(figsize=(12, 8))
for i, y in enumerate(signals):
    plt.subplot(3, 1, i+1)
    librosa.display.waveshow(y, sr=sampling_rates[i], alpha=0.7)
    plt.title(f'Forma de onda - Micr√≥fono {i+1}')
    plt.xlabel('Tiempo (s)')
    plt.ylabel('Amplitud (dB)')
    plt.xticks(np.linspace(0, len(y) / sampling_rates[i], num=5))  # Etiquetas en el eje x
    plt.yticks(np.linspace(np.min(y), np.max(y), num=5))  # Etiquetas en el eje y

plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/4db6ddca-792b-413f-8cf3-adb39022b9dc)

- An√°lisis frecuencial: La Transformada R√°pida de Fourier (FFT) convierte la se√±al al dominio de la frecuencia, mostrando qu√© frecuencias est√°n presentes y su intensidad en decibeles (dB).

    Permite identificar sonidos graves o agudos, analizar ruido y comparar se√±ales de distintos micr√≥fonos.

```python
# An√°lisis Espectral: Aplicar FFT y visualizar espectros de frecuencia
plt.figure(figsize=(12, 8))
for i, y in enumerate(signals):
    N = len(y)  # N√∫mero de muestras
    T = 1.0 / sampling_rates[i]  # Per√≠odo de muestreo
    freqs = np.fft.rfftfreq(N, T)  # Frecuencias
    fft_vals = 20 * np.log10(np.abs(np.fft.rfft(y)) + 1e-6)  # Convertir amplitud a dB  # Magnitud de la FFT

    plt.subplot(3, 1, i+1)
    plt.plot(freqs, fft_vals, label=f'Espectro - Micr√≥fono {i+1}')
    plt.xlim(0, 5000)  # Limitar a 5 kHz (frecuencias de voz humana)
    plt.xlabel('Frecuencia (Hz)')
    plt.ylabel('Amplitud (dB)')
    plt.xticks(np.linspace(0, 5000, num=6))  # Etiquetas en el eje x
    plt.yticks(np.linspace(np.min(fft_vals), np.max(fft_vals), num=5))  # Etiquetas en el eje y
    plt.legend()

plt.tight_layout()
plt.show()
```
![image](https://raw.githubusercontent.com/Maria-Paula05/Laboratorio-3/refs/heads/main/FFT.png)

# 4. M√©todos de separaci√≥n de fuentes
# ICA
*"El An√°lisis de Componentes Independientes (ICA) es una t√©cnica estad√≠stica que permite separar se√±ales mezcladas asumiendo que sus fuentes son estad√≠sticamente independientes. Se utiliza en el procesamiento de audio para aislar sonidos individuales a partir de una mezcla." (Learn Statistics Easily, 2024).*

El proceso para separar fuentes comienza con la carga de archivos de audio, asegurando que todas las se√±ales mezcladas tengan la misma frecuencia de muestreo y longitud. Luego, se aplica el An√°lisis de Componentes Independientes (ICA) para separar las fuentes sonoras, asumiendo que son estad√≠sticamente independientes.
Para mejorar la calidad del audio extra√≠do, se utiliza un filtro pasa banda (100 Hz - 4000 Hz), eliminando frecuencias no deseadas. Luego, la se√±al se normaliza y convierte al formato int16, optimiz√°ndola para su almacenamiento. Finalmente, el archivo se guarda en formato .wav.

```python

```
**Comparaci√≥n se√±al aislada / la se√±al original**
Por ultimo se compara la se√±al aislada con la se√±al original mediante m√©tricas de calidad, para cuantificar el desempe√±o de la separaci√≥n y evaluar la efectividad del proceso.

# SNR

Es una medida que compara el nivel de una se√±al deseada con el nivel del ruido de fondo en un sistema de audio o comunicaciones. Se expresa generalmente en decibeles (dB)
                                                                                               SNR=10log10(Pse√±al/Pruido)
                                                                                               
Un SNR alto (mayor cantidad de dB) significa que la se√±al es mucho m√°s fuerte que el ruido, lo que resulta en una mejor calidad de audio, mientras que un SNR bajo (menor cantidad de dB) indica que el ruido es comparable o incluso mayor que la se√±al, lo que puede dificultar la interpretaci√≥n de la informaci√≥n transmitida.

# 5. Preguntas a resolver
**¬øC√≥mo afecta la posici√≥n relativa de los micr√≥fonos y las fuentes sonoras en la efectividad de la separaci√≥n de se√±ales?**

- Posici√≥n relativa de los micr√≥fonos: la posici√≥n de los micr√≥fonos y las fuentes sonoras afecta significativamente la efectividad en la separaci√≥n de se√±ales debido a la distancia, la direccionalidad y el rechazo de interferencias.

- Distancia y √°ngulo: Un mayor espaciamiento entre micr√≥fonos mejora la capacidad de separar se√±ales basadas en la diferencia de tiempo de llegada y la fase.

- Rechazo de ruido: Un correcto posicionamiento minimiza interferencias y maximiza la captaci√≥n de la se√±al deseada.

- Efecto de diafon√≠a: Si las fuentes est√°n muy cercanas o alineadas, puede ser m√°s dif√≠cil separar sus se√±ales debido a la superposici√≥n de frecuencias.

- Ambiente ac√∫stico: La reverberaci√≥n y los reflejos pueden alterar la captaci√≥n, por lo que la ubicaci√≥n de los micr√≥fonos debe optimizarse para minimizar estos efectos.

**¬øQu√© mejoras implementar√≠a en la metodolog√≠a para obtener mejores resultados?**

- Optimizaci√≥n de la disposici√≥n espacial

    Aumentar la separaci√≥n entre micr√≥fonos para mejorar la diferenciaci√≥n de las se√±ales seg√∫n la diferencia de tiempo de llegada.
    Ubicar los micr√≥fonos estrat√©gicamente para minimizar interferencias y reflexiones no deseadas.

- Reducci√≥n de ruido y diafon√≠a

    Utilizar micr√≥fonos direccionales o configuraciones de matrices para mejorar la captaci√≥n de la se√±al deseada y reducir interferencias.
    Implementar filtros de acondicionamiento de se√±ales para minimizar el ruido ambiental.

- Calibraci√≥n y caracterizaci√≥n precisa

    Realizar pruebas previas de caracterizaci√≥n de los micr√≥fonos para identificar su respuesta en diferentes condiciones.
    Aplicar t√©cnicas de normalizaci√≥n y ajuste de ganancia para mantener niveles de se√±al √≥ptimos.

- Mejora en el procesamiento de se√±ales

    Aplicar t√©cnicas avanzadas de separaci√≥n de fuentes como algoritmos de an√°lisis de componentes independientes (ICA) o filtrado adaptativo.
    Implementar modelos de machine learning o t√©cnicas de deep learning para mejorar la separaci√≥n de se√±ales en entornos ruidosos.

- Simulaci√≥n y pruebas experimentales

    Realizar simulaciones previas mediante software como MATLAB o Proteus para ajustar los par√°metros del sistema antes de la implementaci√≥n real.
    Efectuar mediciones en diferentes condiciones para evaluar la robustez del m√©todo y ajustar los par√°metros necesarios.
  
# Referencias

-   Transformaci√≥n r√°pida de Fourier FFT (n.d.).
https://www.nti-audio.com/es/servicio/conocimientos/transformacion-rapida-de-fourier-fft

-   Rivera, E., Moreno, R., P√©rez, H., & Nakano, M. (2020). Separaci√≥n de se√±ales usando an√°lisis de componentes principales y muestreo compresivo con mediciones m√≠nimas. CIT Informacion Tecnologica, 31(1), 287‚Äì300. https://doi.org/10.4067/s0718-07642020000100287

-   Acaro, X., Molina, M., Corapi, P., Molina Villacis, M., Reamache Rivera, G. J., & Castillo Garc√≠a, J. V. (2021). Interfaz gr√°fica para el an√°lisis de audio y sonidos urbanos (CasRem). Investigaci√≥n, Tecnolog√≠a e Innovaci√≥n, 13(14), 29‚Äì42. https://doi.org/10.53591/iti.v13i14.1308

-   Marengo Rodriguez, F. A., Roveri, E. A., Guerrero, J. M. R., Treffil√≥, M., & Miyara, F. (s/f). AN√ÅLISIS COMPARATIVO DE CODIFICADORES DE AUDIO SIN P√âRDIDAS. Edu.ar. Recuperado el 28 de febrero de 2025, de https://www.fceia.unr.edu.ar/acustica/codecdigital/archivos/comparativo-codificadores-sin-perdidas-UNTREF.pdf

